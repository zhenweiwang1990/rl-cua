# CUA Agent Environment Configuration

# GBox API Key (required)
# Get your API key from https://gbox.ai
GBOX_API_KEY=your_gbox_api_key_here

# ========================================
# VLM Provider Configuration
# ========================================

# VLM Provider: "vllm" or "openrouter" (default: vllm)
# - "vllm": Use local vLLM server or load model locally
# - "openrouter": Use OpenRouter API (requires OPENROUTER_API_KEY)
VLM_PROVIDER=vllm

# vLLM Server URL (optional, used when VLM_PROVIDER=vllm)
# If not set, the agent will load the model locally
# For server mode, start vLLM with ./scripts/run_vllm_base.sh
VLLM_API_BASE=http://localhost:8000/v1

# OpenRouter API Key (required when VLM_PROVIDER=openrouter)
# Get your API key from https://openrouter.ai
OPENROUTER_API_KEY=your_openrouter_api_key_here

# OpenRouter API Base URL (optional, default: https://openrouter.ai/api/v1)
OPENROUTER_API_BASE=https://openrouter.ai/api/v1

# Model name (default: unsloth/Qwen3-VL-30B-A3B-Instruct)
# For OpenRouter, the model will be automatically mapped to OpenRouter format:
#   - unsloth/Qwen3-VL-30B-A3B-Instruct -> qwen/qwen3-vl-30b-a3b-instruct (recommended, available on OpenRouter)
#   - unsloth/Qwen3-VL-32B-Instruct -> qwen/qwen3-vl-32b-instruct (may be unavailable)
#   - unsloth/Qwen3-VL-8B-Instruct -> qwen/qwen3-vl-8b-instruct
#   - Qwen/Qwen2.5-VL-32B-Instruct -> qwen/qwen2.5-vl-32b-instruct
# 
# Recommended: Use Qwen3-VL-30B-A3B-Instruct for OpenRouter (available at https://openrouter.ai/qwen/qwen3-vl-30b-a3b-instruct)
# For vLLM, use: unsloth/Qwen3-VL-30B-A3B-Instruct (from ModelScope: https://www.modelscope.ai/models/unsloth/Qwen3-VL-30B-A3B-Instruct)
MODEL_NAME=unsloth/Qwen3-VL-30B-A3B-Instruct

# Box type (default: android)
BOX_TYPE=android

# Maximum turns (default: 20)
MAX_TURNS=20

# ========================================
# Model Hub Configuration
# ========================================

# Model hub to use: "huggingface" or "modelscope"
# ModelScope is faster for users in China
MODEL_HUB=huggingface

# Hugging Face mirror (for China users)
# Default: https://hf-mirror.com
# Original: https://huggingface.co
HF_ENDPOINT=https://hf-mirror.com

# ModelScope cache directory (when using MODEL_HUB=modelscope)
MODELSCOPE_CACHE=$HOME/.cache/modelscope

# ========================================
# GRPO Training Configuration
# ========================================

# Training hyperparameters
LEARNING_RATE=1e-5
BETA=0.01  # KL penalty coefficient
CLIP_EPSILON=0.2  # PPO clip parameter
BATCH_SIZE=4  # Number of tasks per training batch
GRADIENT_ACCUMULATION_STEPS=1
MAX_GRAD_NORM=1.0

# Training schedule
MAX_STEPS=200  # Total training steps
WARMUP_STEPS=10
EVAL_STEPS=10  # Evaluate every N steps
SAVE_STEPS=10  # Save checkpoint every N steps
SAVE_TOTAL_LIMIT=5  # Keep last N checkpoints

# Early stopping
TARGET_ACCURACY=0.80  # Stop when accuracy reaches this
PATIENCE=5  # Stop after N evals without improvement

# GRPO specific
NUM_ROLLOUTS=4  # Rollouts per task (GRPO group size)
MIN_GROUP_STD=0.05  # Minimum std to keep a group

# Rollout collection
MAX_TURNS=15  # Maximum turns per rollout
ROLLOUT_CONCURRENCY=4  # Parallel rollouts

# LoRA configuration
LORA_R=16  # LoRA rank
LORA_ALPHA=32  # LoRA alpha
LORA_DROPOUT=0.0

# Dynamic LoRA switching
ENABLE_DYNAMIC_LORA=true  # Update vLLM LoRA during training
LORA_UPDATE_STEPS=10  # Update LoRA every N steps

# Model settings
MAX_SEQ_LENGTH=16384
LOAD_IN_4BIT=true  # Use 4-bit quantization (REQUIRED for single GPU setup)

# Single GPU Configuration (if you only have 1 GPU):
# - Set GPU_MEMORY_UTILIZATION=0.4-0.5 for vLLM (leave room for training)
# - Reduce MAX_MODEL_LEN=8192 for vLLM
# - Consider using smaller model: MODEL_NAME=unsloth/Qwen3-VL-8B-Instruct
# - Reduce BATCH_SIZE=2 and NUM_ROLLOUTS=2

# Output
OUTPUT_DIR=outputs/grpo_cua
SEED=42

# Logging
VERBOSE=false
ENABLE_DETAILED_LOGGING=false

# Weights & Biases (optional)
ENABLE_WANDB=true
WANDB_PROJECT=cua-grpo
WANDB_ENTITY=  # Your W&B entity (optional)
WANDB_MODE=online  # online, offline, or disabled

