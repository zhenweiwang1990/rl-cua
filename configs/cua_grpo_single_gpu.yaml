# configs/cua_grpo_single_gpu.yaml
# CUA Agent GRPO 训练配置 - 单 GPU (H200) 优化版
# 基于 AReal 官方示例格式: https://github.com/inclusionAI/AReaL/blob/main/examples/lora/gsm8k_grpo_lora_vllm.yaml

experiment_name: cua-grpo-single-gpu
trial_name: trial0

seed: 42
enable_offload: false
total_train_epochs: 10
tokenizer_path: ${actor.path}

cluster:
  n_nodes: 1
  n_gpus_per_node: 1
  fileroot: /workspace/outputs/grpo_cua_single_gpu
  name_resolve:
    type: nfs
    nfs_record_root: /workspace/outputs/grpo_cua_single_gpu/name_resolve

# AReal 官方推荐：使用内置 vLLM 服务（由 AReal 自动管理）
# 格式：vllm:d{devices}p{processes}t{threads}+d{devices}p{processes}t{threads}
# 第一部分是 vLLM 服务器，第二部分是训练器
allocation_mode: vllm:d1p1t1+d1p1t1

rollout:
  experiment_name: ${experiment_name}
  trial_name: ${trial_name}
  max_concurrent_rollouts: 8  # = batch_size × n_samples = 2 × 4 concurrent GBox VMs
  queue_size: null
  consumer_batch_size: ${train_dataset.batch_size}
  max_head_offpolicyness: 2
  enable_rollout_tracing: false
  use_lora: false  # 全参训练，不使用 LoRA

gconfig:
  # n_samples: Number of PARALLEL independent episodes per task for GRPO
  # 
  # For CUA Agent, each task runs n_samples episodes in separate GBox environments.
  # This enables true GRPO group-wise comparison:
  #   - Same task, different rollouts → compare which episode performed better
  #   - Advantages computed within each group of n_samples episodes
  #
  # Resource usage: n_samples GBox VMs run concurrently per task
  # Example: batch_size=4, n_samples=2 → 4 tasks × 2 episodes = 8 GBox VMs
  #
  # Recommended settings:
  #   - n_samples=2: Good balance of GRPO benefits vs resource usage
  #   - n_samples=4: Better GRPO signal, but 4x GBox environments
  n_samples: 4  # Each task runs 4 parallel episodes for GRPO group comparison
  min_new_tokens: 0
  max_new_tokens: 512  # 降低 max_new_tokens 以加快生成速度（工具调用通常不需要太长输出）
  greedy: false
  temperature: 0.7

actor:
  experiment_name: ${experiment_name}
  trial_name: ${trial_name}
  path: Qwen/Qwen3-VL-8B-Instruct
  init_from_scratch: false
  disable_dropout: true
  gradient_checkpointing: true
  dtype: bfloat16
  mb_spec:
    # 降低 microbatch 大小以节省显存
    max_tokens_per_mb: 4096
  optimizer:
    type: adam
    lr: 1.0e-5
    weight_decay: 0.01
    beta1: 0.9
    beta2: 0.999
    eps: 1e-8
    lr_scheduler_type: cosine
    gradient_clipping: 1.0
    warmup_steps_proportion: 0.05
  group_size: ${gconfig.n_samples}
  eps_clip: 0.2
  temperature: ${gconfig.temperature}
  reward_scaling: 1.0
  reward_bias: 0.0
  kl_ctl: 0.01
  ppo_n_minibatches: 1
  recompute_logprob: true
  use_decoupled_loss: true
  behav_imp_weight_cap: 5.0
  dynamic_sampling: false
  reward_norm:
    mean_level: group
    std_level: group
    group_size: ${gconfig.n_samples}
  adv_norm:
    mean_level: batch
    std_level: batch
  max_new_tokens: ${gconfig.max_new_tokens}
  weight_update_mode: xccl
  
  # 全参训练：不使用 LoRA，直接训练完整模型
  # 优点：避免 LoRA 兼容性问题，训练更稳定
  # 缺点：需要更多显存，单 GPU 可能需要更小的 batch size
  use_lora: false

ref:
  experiment_name: ${experiment_name}
  trial_name: ${trial_name}
  path: ${actor.path}
  init_from_scratch: false
  disable_dropout: true
  dtype: ${actor.dtype}
  mb_spec:
    # 降低 microbatch 大小以节省显存
    max_tokens_per_mb: 4096
  optimizer: null

# vLLM 配置（内置 vLLM 服务器，由 AReal 自动启动和管理）
# 全参训练：vLLM 加载完整模型，直接接收全参权重更新
# 注意：单 GPU 模式下，vLLM 和训练器共享显存，需要合理分配
vllm:
  model: ${actor.path}
  seed: ${seed}
  skip_tokenizer_init: false
  dtype: ${actor.dtype}
  max_model_len: 4096  # 降低 max_model_len 以提升推理速度并节省显存
  # 提高 vLLM 内存使用率以提升批处理能力和推理速度
  # 0.5 = 约 70GB，为训练器预留约 60GB（模型+优化器状态）
  # 如果显存不足，可以降低到 0.4-0.45
  gpu_memory_utilization: 0.5
  # 全参训练：不使用 LoRA
  enable_lora: false
  # 禁用 enforce_eager 以启用 CUDA 图优化，大幅提升推理速度
  # 注意：如果遇到兼容性问题，可以改回 true
  enforce_eager: false
  # 禁用 sliding window 与 rope_scaling 不兼容，必须设为 false
  disable_sliding_window: false
  # 注意：工具调用支持需要在 vLLM 启动时通过命令行参数传递
  # AReaL 的 vLLMConfig 不支持 enable_auto_tool_choice 和 tool_call_parser
  # 如果 vLLM 不支持原生 tool_calls，workflow 会自动回退到 JSON 解析模式

# datasets
# IMPORTANT: For CUA training with n_samples > 1:
#   - batch_size = number of TASKS per training step
#   - Each task produces n_samples episodes
#   - Effective samples per step = batch_size × n_samples
#
# Example: batch_size=2, n_samples=4 → 2 tasks × 4 episodes = 8 samples
# The granularity check: (batch_size × n_samples) % n_samples = 0 ✓
#
# Resource usage: batch_size × n_samples = concurrent GBox VMs
train_dataset:
  batch_size: 2  # 2 tasks × 4 episodes/task = 8 samples per batch, 8 GBox VMs
  shuffle: true
  pin_memory: true
  num_workers: 2
  drop_last: true  # Ensure all batches have the same size, divisible by group_size
  path: cua_agent.tasks:get_areal_train_dataset
  type: rl
  max_length: 8192

valid_dataset:
  batch_size: 2
  shuffle: false
  pin_memory: true
  num_workers: 2
  path: cua_agent.tasks:get_areal_eval_dataset
  type: rl
  max_length: 8192

# Utilities
saver:
  experiment_name: ${experiment_name}
  trial_name: ${trial_name}
  fileroot: ${cluster.fileroot}
  freq_epochs: 1
  freq_steps: 10
  freq_secs: null

recover:
  mode: disabled
  experiment_name: ${experiment_name}
  trial_name: ${trial_name}
  fileroot: ${cluster.fileroot}
  freq_epochs: 1
  freq_steps: null
  freq_secs: 3600

evaluator:
  experiment_name: ${experiment_name}
  trial_name: ${trial_name}
  fileroot: ${cluster.fileroot}
  freq_epochs: 1
  freq_steps: 10
  freq_secs: null

stats_logger:
  experiment_name: ${experiment_name}
  trial_name: ${trial_name}
  fileroot: ${cluster.fileroot}
  wandb:
    mode: disabled
    project: cua-grpo-single-gpu
    entity: null

launcher:
  inference_server_cpus_per_gpu: 4
  inference_server_mem_per_gpu: 16384
  trainer_cpus_per_gpu: 4
  trainer_mem_per_gpu: 16384
