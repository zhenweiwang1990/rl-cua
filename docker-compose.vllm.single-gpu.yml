# docker-compose.vllm.single-gpu.yml
# vLLM 服务独立配置（单 GPU，持续运行）

services:
  # ============ vLLM 推理服务 ============
  vllm:
    image: vllm/vllm-openai:latest
    container_name: vllm-cua-areal-single
    
    # 使用单个 GPU
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./outputs:/workspace/outputs
    
    environment:
      - VLLM_ALLOW_RUNTIME_LORA_UPDATING=True
      - MODEL_NAME=${MODEL_NAME:-Qwen/Qwen3-VL-8B-Instruct}
      - TENSOR_PARALLEL_SIZE=1
      - MAX_MODEL_LEN=${MAX_MODEL_LEN:-8192}
      - GPU_MEMORY_UTILIZATION=${GPU_MEMORY_UTILIZATION:-0.5}
    
    # vLLM 启动命令（单 GPU 配置）
    command: >
      --model ${MODEL_NAME:-Qwen/Qwen3-VL-8B-Instruct}
      --tensor-parallel-size 1
      --max-model-len ${MAX_MODEL_LEN:-8192}
      --gpu-memory-utilization ${GPU_MEMORY_UTILIZATION:-0.5}
      --enable-lora
      --max-lora-rank 16
      --trust-remote-code
      --max-num-seqs 4
      --max-num-batched-tokens 2048
    
    ports:
      - "8000:8000"
    
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s  # 给 vLLM 足够的启动时间
    
    # 始终重启（即使容器退出）
    restart: always
    
    # 网络配置
    networks:
      - vllm-network

networks:
  vllm-network:
    driver: bridge

